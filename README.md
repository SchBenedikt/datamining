# ğŸŒ Purpose & Functionality

The **Heise News Crawler** is designed to automatically extract and store news articles from Heise's archive. The primary goals are:

- ğŸ“¡ **Data Collection:** Gather historical news articles from Heise.de.
- ğŸ› **Structured Storage:** Store articles in a PostgreSQL database for easy querying and analysis.
- ğŸ” **Metadata Extraction:** Retrieve key information such as title, author, category, keywords, and word count.
- ğŸ”„ **Incremental crawling:** Detect duplicate articles and save only new articles of the current day.
- ğŸ”” **Notifications:** Send an email if an error occurs during the crawling process.
- ğŸ¨ **Enhanced Terminal Output:** Uses PyFiglet for improved readability.
- ğŸ“¤ **Data export:** Export of articles as .csv, .json, .xlsx-file or display the data in a stats.html file
- ğŸ–¥ **API**: Provision of statistics and complete data sets.
  
Also an API endpoint is provided that can display the crawled data and statistics.

---

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Requirements

ğŸ”¹ Python 3

ğŸ”¹ PostgreSQL

ğŸ”¹ Required Python Libraries (Dependencies in [requirements.txt](requirements.txt))

### 2ï¸âƒ£ Install Dependencies

Install required Python libraries:

```sh
pip install -r requirements.txt
```

### 3ï¸âƒ£ Create `.env` File

Set up your database and email credentials by creating a `.env` file:

```sh
   EMAIL_USER=...
   EMAIL_PASSWORD=...
   SMTP_SERVER=...
   SMTP_PORT=...
   ALERT_EMAIL=...

   DB_NAME=...
   DB_USER=...
   DB_PASSWORD=...
   DB_HOST=...
   DB_PORT=...
```


---

## ğŸ›  Usage

### 1ï¸âƒ£ Start the first Crawler (into the past)

```sh
python3 main.py
```

#### Example Terminal Output

```
[INFO] Crawle URL: https://www.heise.de/newsticker/archiv/xxxx/xx
[INFO] Gefundene Artikel (insgesamt): 55
xxxx-xx-xx xx:xx:xx [INFO] Verarbeite 16 Artikel fÃ¼r den Tag xxxx-xx-xx
xxxx-xx-xx xx:xx:xx [INFO] 2025-03-01T20:00:00 - article-name
(â¬†ï¸ date)
```
If fewer than 10 items are found per day, an e-mail will be sent


### 2ï¸âƒ£ Start the second Crawler (for current articles in the present)

```sh
python3 current_crawler.py
```

#### Example Terminal Output

```
[INFO] Crawle URL: https://www.heise.de/newsticker/archiv/xxxx/xx
[INFO] Gefundene Artikel (insgesamt): 55
xxxx-xx-xx xx:xx:xx [INFO] Aktueller Crawl-Durchlauf abgeschlossen.
xxxx-xx-xx xx:xx:xx [INFO] Warte 300 Sekunden bis zum nÃ¤chsten Crawl.
(â¬†ï¸ date)
```

### 3ï¸âƒ£ Use API

The API server starts automatically. You can call up the statistics here:
```
http://127.0.0.1:6600/stats
```


### 4ï¸âƒ£ Export articles

You can export the data for each item to a CSV, JSON or XLSX file.
```sh
python3 export_articles.py
```
Exported articles are saved in the current directory.

---




---

## ğŸ— Database Schema

| Column       | Type   | Description          |
| ------------ | ------ | -------------------- |
| id           | SERIAL | Unique ID            |
| title        | TEXT   | Article title        |
| url          | TEXT   | Article URL (unique) |
| date         | TEXT   | Publication date     |
| author       | TEXT   | Author(s)            |
| category     | TEXT   | Category             |
| keywords     | TEXT   | Keywords             |
| word\_count  | INT    | Word count           |
| editor\_abbr | TEXT   | Editor abbreviation  |
| site\_name   | TEXT   | Website name         |

---



## ğŸ“© Error Notifications

If any errors occur, an email notification will be sent.

---

## ğŸ“‚ Project Structure

```
ğŸ“‚ Heise-News-Crawler
â”œâ”€â”€ ğŸ“„ .gitignore                 # Git ignore file
â”œâ”€â”€ ğŸ“„ .env                       # Environment variables (email & database config, you have to create this file manually)
â”œâ”€â”€ ğŸ“„ main.py                    # Main crawler script
â”œâ”€â”€ ğŸ“„ api.py                     # API functionalities
â”œâ”€â”€ ğŸ“„ notification.py            # Email notification handler
â”œâ”€â”€ ğŸ“„ test_notifications.py      # Testing email notifications
â”œâ”€â”€ ğŸ“„ README.md                  
â”œâ”€â”€ ğŸ“„ current_crawler.py         # Crawler for newer articles
â”œâ”€â”€ ğŸ“„ export_articles.py         # Function to export the data
â”œâ”€â”€ ğŸ“„ requirements.txt           
â””â”€â”€ ğŸ“‚ templates/                 # HTML email templates
    â”œâ”€â”€ ğŸ“„ stats.html             # API functionalities
â””â”€â”€ ğŸ“‚ data/                      # Export data (as of 03/03/2025)
    â”œâ”€â”€ ğŸ“„ .gitattributes         
    â”œâ”€â”€ ğŸ“„ README.md
    â”œâ”€â”€ ğŸ“„ api.py             
    â”œâ”€â”€ ğŸ“„ articles_export.csv
    â”œâ”€â”€ ğŸ“„ articles_export.json
    â”œâ”€â”€ ğŸ“„ articles_export.xlsx
â””â”€â”€ ğŸ“„ LICENCE  
```

## â—Troubleshooting

### ğŸŒ Start API manually

```sh
python3 api.py
```

### ğŸ“§ Testing Notifications

```sh
python3 test_notification.py
```

### âš ï¸ Found an error?
Please create a pull request or contact us via server@schÃ¤chner.de

---


## ğŸ“œ License
This program is licensed under **GNU GENERAL PUBLIC LICENSE**



## ğŸ™‹ About us

This project was programmed by both of us within a few days:
- https://github.com/schBenedikt
- https://github.com/schVinzenz

### ğŸ“¬ Contact

Feel free to reach out if you have any questions, feedback, or just want to say hi!

ğŸ“§ Email: [server@schÃ¤chner.de](mailto:server@schÃ¤chner.de)

ğŸŒ Website:
- https://technik.schÃ¤chner.de
- https://benedikt.schÃ¤chner.de
- https://vinzenz.schÃ¤chner.de


ğŸ’– Special Thanks

The idea for our Heise News Crawler comes from David Kriesel and his presentation â€œSpiegel Miningâ€ at 33c3.


---

Happy Crawling! ğŸ‰
