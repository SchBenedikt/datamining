<a href="https://discord.com/invite/Q6Nn2z3tUP">
  <img src="https://discord.com/api/guilds/1346160903304773703/widget.png?style=banner2" width="250"/>
</a>
<a href="https://deepnote.com/app/schachner/Web-Crawler-d5025a36-3829-4c12-ad2d-b81aa84bd217?utm_source=app-settings&utm_medium=product-embed&utm_campaign=data-app&utm_content=d5025a36-3829-4c12-ad2d-b81aa84bd217&__embedded=true">
  <img src="https://img.shields.io/badge/Open%20in-Deepnote-blue?style=for-the-badge&logo=deepnote" width="250"/>
</a>

# ğŸ—ï¸ Unified News Mining System

Ein umfassendes, einheitliches Crawler-System zum Sammeln und Analysieren von Nachrichtenartikeln von **Heise.de** und **Chip.de**.

---

## ğŸ“‘ Inhaltsverzeichnis

- [ğŸ¯ Quick Links](#-quick-links)
- [âœ¨ Key Features](#-key-features)
- [ğŸŒ Zweck & FunktionalitÃ¤t](#-zweck--funktionalitÃ¤t)
- [ğŸš€ Installation & Setup](#-installation--setup)
  - [Voraussetzungen](#1ï¸âƒ£-voraussetzungen)
  - [Repository klonen](#2ï¸âƒ£-repository-klonen)
  - [Dependencies installieren](#3ï¸âƒ£-dependencies-installieren)
  - [Umgebungsvariablen konfigurieren](#4ï¸âƒ£-umgebungsvariablen-konfigurieren)
  - [Datenbank Setup](#5ï¸âƒ£-datenbank-setup)
- [ğŸ›  Verwendung](#-verwendung)
  - [Crawler starten](#crawler-starten)
  - [Streamlit Dashboard](#streamlit-dashboard)
  - [Discord Bot](#discord-bot)
  - [API Endpoints](#api-endpoints)
  - [Daten exportieren](#daten-exportieren)
- [ğŸ³ Docker Deployment](#-docker-deployment)
- [ğŸ— Datenbankschema](#-datenbankschema)
- [ğŸ“Š Streamlit Features](#-streamlit-features)
- [ğŸ“‚ Projektstruktur](#-projektstruktur)
- [ğŸ”§ Verwaltung mit Docker-Tools](#-verwaltung-mit-docker-tools)
- [â— Troubleshooting](#-troubleshooting)
- [ğŸ—‚ï¸ Beispiele & Screenshots](#ï¸-beispiele--screenshots)
- [ğŸ“œ Lizenz](#-lizenz)
- [ğŸ™‹ Ãœber uns](#-Ã¼ber-uns)

---

## ğŸ¯ Quick Links

- ğŸ“– **[Quick Start Guide](QUICKSTART.md)** - In 5 Minuten starten
- ğŸ—ï¸ **[Architecture](ARCHITECTURE.md)** - Systemarchitektur und Datenfluss
- ğŸ³ **[Docker Setup](DOCKER_SETUP.md)** - Deployment mit Docker

---

## âœ¨ Key Features

- âœ… **Separate Tabellen** - Zwei dedizierte PostgreSQL-Tabellen (heise & chip)
- âœ… **Live Monitoring** - Automatische PrÃ¼fung alle 5-10 Minuten
- âœ… **Single Dashboard** - Eine Streamlit-App fÃ¼r beide Quellen
- âœ… **Source Filtering** - Anzeige von Heise, Chip oder beiden
- âœ… **Discord Bot** - Echtzeit-Statistik-Updates
- âœ… **Daten exportieren** - CSV, Excel, JSON fÃ¼r beide Quellen
- âœ… **Docker Ready** - Deployment mit einem Befehl
- âœ… **AI Analytics** - Powered by Google Generative AI
- âœ… **Autoren-Netzwerke** - Visualisierung von Autoren-Verbindungen
- âœ… **Keyword-Analysen** - Schlagwortverteilung und Trends
- âœ… **Zeitanalysen** - Zeitbasierte Diagramme und Graphen
- âœ… **Suchfunktion** - Volltext-Suche in allen Artikeln
- âœ… **Filterfunktionen** - Nach Quelle, Datum, Autor, Kategorie

---

## ğŸŒ Zweck & FunktionalitÃ¤t

Das **News Mining System** ist darauf ausgelegt, automatisch Nachrichtenartikel aus mehreren Quellen zu extrahieren und zu speichern. Die Hauptziele sind:

- ğŸ“¡ **Datensammlung** - Erfassung historischer Nachrichtenartikel von Heise.de und Chip.de
- ğŸ› **Strukturierte Speicherung** - Artikel beider Quellen in separaten PostgreSQL-Tabellen
- ğŸ” **Metadaten-Extraktion** - Erfassung von Titel, Autor, Kategorie, SchlagwÃ¶rtern, Wortanzahl und mehr
- ğŸ”„ **Inkrementelles Crawling** - Erkennung von Duplikaten und Speicherung nur neuer Artikel
- ğŸ”” **Benachrichtigungen** - E-Mail-Benachrichtigung bei Fehlern wÃ¤hrend des Crawling-Prozesses
- ğŸ¨ **Verbesserte Terminal-Ausgabe** - Nutzung von PyFiglet fÃ¼r bessere Lesbarkeit
- ğŸ“¤ **Datenexport** - Export als CSV, JSON, XLSX mit Quellenfilterung
- ğŸ–¥ **API** - Bereitstellung von Statistiken und kompletten DatensÃ¤tzen
- ğŸ¤– **AI Analytics** - Erweiterte Analysen mit Google Generative AI fÃ¼r Topic Modeling, Sentiment Analysis und Trend Detection
- ğŸ¯ **Einheitliches Dashboard** - Eine Streamlit-Anwendung fÃ¼r beide Quellen
- ğŸ¤– **Discord Bot** - Echtzeit-Statistiken fÃ¼r beide Quellen in Discord
- ğŸ“Š **Umfangreiche Visualisierungen** - Ãœber 20 verschiedene Diagramme, Graphen und Darstellungen
- ğŸ•¸ï¸ **Autoren-Netzwerke** - Visualisierung von Verbindungen zwischen Autoren
- ğŸ“ˆ **Trend-Analysen** - Zeitbasierte Analysen und Vorhersagen

Ein API-Endpoint wird ebenfalls bereitgestellt, der die gecrawlten Daten und Statistiken anzeigen kann.

---

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Voraussetzungen

ğŸ”¹ **Python 3.8+** (empfohlen: Python 3.11)

ğŸ”¹ **PostgreSQL 13+** (lokal oder remote)

ğŸ”¹ **Git** (fÃ¼r das Klonen des Repositories)

ğŸ”¹ **pip3** (Python Package Manager)

Optional:
- ğŸ³ **Docker & Docker Compose** (fÃ¼r containerisiertes Deployment)
- ğŸ® **Discord Bot Token** (fÃ¼r Discord-Integration)
- ğŸ¤– **Google API Key** (fÃ¼r KI-Analysen)

### 2ï¸âƒ£ Repository klonen

```bash
git clone https://github.com/SchBenedikt/datamining.git
cd datamining
```

### 3ï¸âƒ£ Dependencies installieren

Installieren Sie alle erforderlichen Python-Bibliotheken:

```bash
pip3 install -r requirements.txt
```

FÃ¼r die Streamlit-Anwendung (erweiterte Visualisierungen):

```bash
cd visualization
pip3 install -r requirements_streamlit.txt
cd ..
```

### 4ï¸âƒ£ Umgebungsvariablen konfigurieren

Erstellen Sie eine `.env`-Datei im Hauptverzeichnis mit folgenden Variablen:

```env
# Datenbank-Konfiguration
DB_NAME=your_database_name
DB_USER=your_database_user
DB_PASSWORD=your_database_password
DB_HOST=localhost
DB_PORT=5432

# E-Mail-Benachrichtigungen (optional)
EMAIL_USER=your_email@example.com
EMAIL_PASSWORD=your_app_password
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
ALERT_EMAIL=recipient@example.com

# Discord Bot (optional)
DISCORD_TOKEN=your_discord_bot_token
CHANNEL_ID=your_discord_channel_id

# Google AI (optional, fÃ¼r erweiterte Analysen)
GOOGLE_API_KEY=your_google_api_key
```

**Hinweise:**
- FÃ¼r Gmail verwenden Sie ein [App-Passwort](https://support.google.com/accounts/answer/185833)
- Discord Token erhalten Sie im [Discord Developer Portal](https://discord.com/developers/applications)
- Google API Key erstellen Sie in der [Google Cloud Console](https://console.cloud.google.com)

### 5ï¸âƒ£ Datenbank Setup

Erstellen Sie die PostgreSQL-Datenbank:

```bash
# PostgreSQL-Konsole Ã¶ffnen
psql -U postgres

# Datenbank erstellen
CREATE DATABASE your_database_name;

# Beenden
\q
```

Die benÃ¶tigten Tabellen werden automatisch beim ersten Start der Crawler erstellt.

**Manuelle Tabellenerstellung (optional):**

```sql
-- Heise-Tabelle
CREATE TABLE IF NOT EXISTS heise (
    id SERIAL PRIMARY KEY,
    title TEXT,
    url TEXT UNIQUE,
    date TEXT,
    author TEXT,
    category TEXT,
    keywords TEXT,
    word_count INTEGER,
    editor_abbr TEXT,
    site_name TEXT
);

-- Chip-Tabelle
CREATE TABLE IF NOT EXISTS chip (
    id SERIAL PRIMARY KEY,
    url TEXT UNIQUE,
    title TEXT,
    author TEXT,
    date TEXT,
    keywords TEXT,
    description TEXT,
    type TEXT,
    page_level1 TEXT,
    page_level2 TEXT,
    page_level3 TEXT,
    page_template TEXT
);
```

---

## ğŸ›  Verwendung

### Crawler starten

#### Heise Archive Crawler (crawlt rÃ¼ckwÃ¤rts vom neuesten zum Ã¤ltesten)

```bash
cd heise
python3 main.py
```

**Beispiel Terminal-Ausgabe:**

```
[INFO] Crawle URL: https://www.heise.de/newsticker/archiv/2025/10
[INFO] Gefundene Artikel (insgesamt): 55
2025-10-02 10:30:15 [INFO] Verarbeite 16 Artikel fÃ¼r den Tag 2025-10-02
2025-10-02 10:30:15 [INFO] 2025-10-02T20:00:00 - article-name
```

Falls weniger als 10 Artikel pro Tag gefunden werden, wird eine E-Mail gesendet.

#### Heise Live Crawler (prÃ¼ft alle 5 Minuten auf neue Artikel)

```bash
cd heise
python3 current_crawler.py
```

**Beispiel Terminal-Ausgabe:**

```
[INFO] Crawle URL: https://www.heise.de/newsticker/archiv/2025/10
[INFO] Gefundene Artikel (insgesamt): 55
2025-10-02 10:35:00 [INFO] Aktueller Crawl-Durchlauf abgeschlossen.
2025-10-02 10:35:00 [INFO] Warte 300 Sekunden bis zum nÃ¤chsten Crawl.
```

#### Chip Archive Crawler (crawlt von Seite 1 aufwÃ¤rts)

```bash
cd chip
python3 main.py
```

#### Chip Live Crawler (prÃ¼ft alle 10 Minuten auf neue Artikel)

```bash
cd chip
python3 current_crawler.py
```

---

### Streamlit Dashboard

Starten Sie das interaktive Streamlit-Dashboard mit UnterstÃ¼tzung fÃ¼r beide Quellen:

```bash
cd visualization
streamlit run streamlit_app.py
```

Das Dashboard wird auf `http://localhost:8501` geÃ¶ffnet.

---

### Discord Bot

Starten Sie den Discord Bot fÃ¼r Echtzeit-Statistik-Updates:

```bash
cd heise
python3 bot.py
```

**Der Bot bietet:**
- Gesamtanzahl der Artikel beider Quellen
- Heutige Artikel-Anzahl beider Quellen
- Autoren-Statistiken
- Updates alle 10 Minuten

---

### API Endpoints

Der API-Server startet automatisch beim AusfÃ¼hren von `heise/main.py`. Statistiken kÃ¶nnen hier abgerufen werden:

```
http://127.0.0.1:6600/stats
```

**Manueller Start der API:**

```bash
cd heise
python3 api.py
```

---

### Daten exportieren

Sie kÃ¶nnen die Daten fÃ¼r jede Quelle als CSV, JSON oder XLSX-Datei exportieren.

**Heise-Artikel exportieren:**

```bash
cd heise
python3 export_articles.py
```

**Chip-Artikel exportieren:**

```bash
cd chip
python3 export_articles.py
```

Exportierte Artikel werden im `data/`-Verzeichnis gespeichert.

---

## ğŸ³ Docker Deployment

### Alle Services mit einem Befehl starten

```bash
docker-compose up -d
```

### Einzelne Services verwalten

```bash
# Heise Archive Crawler starten
docker-compose up -d heise-archive-crawler

# Chip Live Crawler starten
docker-compose up -d chip-live-crawler

# Streamlit Dashboard starten
docker-compose up -d streamlit-dashboard

# Discord Bot starten
docker-compose up -d discord-bot
```

### Logs ansehen

```bash
# Alle Services
docker-compose logs -f

# Spezifischer Service
docker-compose logs -f heise-live-crawler
```

### Services stoppen

```bash
# Alle Services stoppen
docker-compose down

# Spezifischer Service
docker-compose stop heise-archive-crawler
```

### Dashboard aufrufen

Nach dem Start ist das Streamlit-Dashboard unter folgender Adresse erreichbar:

```
http://localhost:8501
```

---

## ğŸ— Datenbankschema

Die Datenbank verwendet **zwei separate Tabellen** fÃ¼r bessere Organisation:

### Heise-Tabelle

| Spalte       | Typ    | Beschreibung                |
| ------------ | ------ | --------------------------- |
| id           | SERIAL | Eindeutige ID               |
| title        | TEXT   | Artikel-Titel               |
| url          | TEXT   | Artikel-URL (eindeutig)     |
| date         | TEXT   | VerÃ¶ffentlichungsdatum      |
| author       | TEXT   | Autor(en)                   |
| category     | TEXT   | Kategorie                   |
| keywords     | TEXT   | SchlagwÃ¶rter                |
| word\_count  | INT    | Wortanzahl                  |
| editor\_abbr | TEXT   | Redakteur-KÃ¼rzel            |
| site\_name   | TEXT   | Website-Name                |

### Chip-Tabelle

| Spalte         | Typ    | Beschreibung                |
| -------------- | ------ | --------------------------- |
| id             | SERIAL | Eindeutige ID               |
| url            | TEXT   | Artikel-URL (eindeutig)     |
| title          | TEXT   | Artikel-Titel               |
| author         | TEXT   | Autor(en)                   |
| date           | TEXT   | VerÃ¶ffentlichungsdatum      |
| keywords       | TEXT   | SchlagwÃ¶rter                |
| description    | TEXT   | Artikel-Beschreibung        |
| type           | TEXT   | Artikel-Typ                 |
| page\_level1   | TEXT   | Seitenebene 1               |
| page\_level2   | TEXT   | Seitenebene 2               |
| page\_level3   | TEXT   | Seitenebene 3               |
| page\_template | TEXT   | Seiten-Template             |

**Hinweis:** Das Streamlit-Dashboard fÃ¼hrt Daten aus beiden Tabellen zusammen fÃ¼r einheitliche Ansicht.

---

## ğŸ“Š Streamlit Features

Das Dashboard bietet Ã¼ber **20 verschiedene Funktionen und Visualisierungen**:

### ğŸ“ˆ Visualisierungen

- **Autoren-Netzwerke** (ğŸ•¸ï¸) - Interaktive Netzwerkgraphen zeigen Verbindungen zwischen Autoren
- **Keyword-Analysen** (ğŸ”‘) - HÃ¤ufigkeitsverteilung der wichtigsten SchlagwÃ¶rter
- **Word Clouds** - Visuelle Darstellung der hÃ¤ufigsten Begriffe
- **Zeitanalysen** (ğŸ“…) - Artikel-VerÃ¶ffentlichungen Ã¼ber Zeit
- **Trend-Analysen** - Vorhersagen und Mustererkennungen
- **KI-Analysen** (ğŸ¤–) - Topic Modeling, Sentiment Analysis
- **Sentiment-Analyse** - Stimmungsanalyse der Artikel
- **Topic Clustering** - Automatische Themengruppierung
- **Content-Empfehlungen** - Ã„hnliche Artikel finden
- **Performance-Metriken** (âš¡) - System-Statistiken

### ğŸ”§ Interaktive Features

- **Quellenfilter** - Heise, Chip oder beide anzeigen
- **Suchfunktion** (ğŸ”) - Volltext-Suche in Artikeln
- **Datumsbereich-Filter** - Zeitbasierte Filterung
- **Kategoriefilter** - Nach Kategorie filtern
- **Autorenfilter** - Nach Autor filtern
- **Export-Funktion** - CSV, Excel, JSON
- **SQL-Abfragen** (ğŸ”§) - Eigene Abfragen ausfÃ¼hren
- **Cache-Management** - Daten-Cache leeren

### ğŸ“¥ Export-Optionen

- CSV-Export mit Quelleninfo
- Excel-Export (.xlsx)
- JSON-Export
- SQL-Export
- Gefilterte Exports mÃ¶glich

---

## ğŸ“‚ Projektstruktur

```
ğŸ“‚ datamining/
â”œâ”€â”€ ğŸ“‚ heise/                          # Heise-Crawler und verwandte Skripte
â”‚   â”œâ”€â”€ ğŸ“„ main.py                     # Archive Crawler (rÃ¼ckwÃ¤rts)
â”‚   â”œâ”€â”€ ğŸ“„ current_crawler.py          # Live Crawler (alle 5 Minuten)
â”‚   â”œâ”€â”€ ğŸ“„ bot.py                      # Discord Bot
â”‚   â”œâ”€â”€ ğŸ“„ api.py                      # API-FunktionalitÃ¤ten
â”‚   â”œâ”€â”€ ğŸ“„ notification.py             # E-Mail-Benachrichtigungen
â”‚   â”œâ”€â”€ ğŸ“„ export_articles.py          # Export-FunktionalitÃ¤t
â”‚   â”œâ”€â”€ ğŸ“„ test_notification.py        # Benachrichtigungs-Test
â”‚   â””â”€â”€ ğŸ“‚ templates/                  # HTML-Templates
â”‚       â”œâ”€â”€ ğŸ“„ news_feed.html
â”‚       â””â”€â”€ ğŸ“„ query.html
â”œâ”€â”€ ğŸ“‚ chip/                           # Chip-Crawler und verwandte Skripte
â”‚   â”œâ”€â”€ ğŸ“„ main.py                     # Archive Crawler (vorwÃ¤rts)
â”‚   â”œâ”€â”€ ğŸ“„ current_crawler.py          # Live Crawler (alle 10 Minuten)
â”‚   â”œâ”€â”€ ğŸ“„ notification.py             # E-Mail-Benachrichtigungen
â”‚   â””â”€â”€ ğŸ“„ export_articles.py          # Export-FunktionalitÃ¤t
â”œâ”€â”€ ğŸ“‚ visualization/                  # Einheitliches Streamlit-Dashboard
â”‚   â”œâ”€â”€ ğŸ“„ streamlit_app.py            # Haupt-Streamlit-Anwendung
â”‚   â””â”€â”€ ğŸ“„ requirements_streamlit.txt  # Streamlit-Dependencies
â”œâ”€â”€ ğŸ“‚ data/                           # Export-Verzeichnis
â”œâ”€â”€ ğŸ“‚ docker/                         # Docker-Konfigurationen (falls vorhanden)
â”œâ”€â”€ ğŸ“„ docker-compose.yml              # Docker Compose Konfiguration
â”œâ”€â”€ ğŸ“„ Dockerfile                      # Docker Image Definition
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python-Dependencies
â”œâ”€â”€ ğŸ“„ .env                            # Umgebungsvariablen (manuell erstellen)
â”œâ”€â”€ ğŸ“„ .gitignore                      # Git-Ignore-Datei
â”œâ”€â”€ ğŸ“„ README.md                       # Diese Datei
â”œâ”€â”€ ğŸ“„ QUICKSTART.md                   # Schnellstart-Anleitung
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md                 # Systemarchitektur
â”œâ”€â”€ ğŸ“„ DOCKER_SETUP.md                 # Docker-Setup-Anleitung
â”œâ”€â”€ ğŸ“„ SECURITY.md                     # Sicherheitsrichtlinien
â””â”€â”€ ğŸ“„ LICENSE                         # Lizenz (GNU GPL)
```

---

## ğŸ”§ Verwaltung mit Docker-Tools

FÃ¼r die zentrale Verwaltung Ihrer Docker-Container empfehlen wir folgende 3rd-Party-LÃ¶sungen:

### ğŸ† Portainer (Empfohlen)

**Installation:**

```bash
docker volume create portainer_data

docker run -d \
  -p 9000:9000 \
  --name portainer \
  --restart always \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v portainer_data:/data \
  portainer/portainer-ce:latest
```

**Zugriff:** `http://localhost:9000`

**Features:**
- Web-basierte GUI fÃ¼r Container-Management
- Logs in Echtzeit ansehen
- Container starten/stoppen/pausieren
- Ressourcen-Monitoring
- Stack-Management (Docker Compose)
- Benutzerfreundlich

### ğŸ¨ Dockge (Alternative)

**Installation:**

```bash
docker run -d \
  -p 5001:5001 \
  --name dockge \
  --restart unless-stopped \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v dockge_data:/app/data \
  louislam/dockge:1
```

**Zugriff:** `http://localhost:5001`

**Features:**
- Moderne Alternative zu Portainer
- Docker Compose fokussiert
- Einfache BenutzeroberflÃ¤che
- Live-Logs

### ğŸš¢ Yacht

**Installation:**

```bash
docker volume create yacht

docker run -d \
  -p 8000:8000 \
  --name yacht \
  --restart unless-stopped \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v yacht:/config \
  selfhostedpro/yacht
```

**Zugriff:** `http://localhost:8000`

**Features:**
- Self-hosted Docker-Management
- Template-basiert
- Clean UI

---

## â— Troubleshooting

### Problem: Verbindungsfehler zur Datenbank

**LÃ¶sung:**
1. ÃœberprÃ¼fen Sie die `.env`-Datei auf korrekte Datenbank-Credentials
2. Stellen Sie sicher, dass PostgreSQL lÃ¤uft:
   ```bash
   # macOS
   brew services list
   
   # Linux
   sudo systemctl status postgresql
   ```
3. Testen Sie die Verbindung:
   ```bash
   psql -U $DB_USER -d $DB_NAME -h $DB_HOST
   ```

### Problem: Keine Daten im Streamlit-Dashboard

**LÃ¶sung:**
1. ÃœberprÃ¼fen Sie, ob Tabellen Daten enthalten:
   ```sql
   SELECT COUNT(*) FROM heise;
   SELECT COUNT(*) FROM chip;
   ```
2. LÃ¶schen Sie den Streamlit-Cache mit der SchaltflÃ¤che "ğŸ”„ Cache leeren"
3. Starten Sie die Streamlit-App neu

### Problem: E-Mail-Benachrichtigungen funktionieren nicht

**LÃ¶sung:**
1. FÃ¼r Gmail: Verwenden Sie ein [App-Passwort](https://support.google.com/accounts/answer/185833)
2. Testen Sie die Benachrichtigungsfunktion:
   ```bash
   cd heise
   python3 test_notification.py
   ```
3. ÃœberprÃ¼fen Sie SMTP-Einstellungen in `.env`

### Problem: Discord Bot antwortet nicht

**LÃ¶sung:**
1. ÃœberprÃ¼fen Sie `DISCORD_TOKEN` und `CHANNEL_ID` in `.env`
2. Stellen Sie sicher, dass der Bot die richtigen Permissions hat
3. ÃœberprÃ¼fen Sie die Bot-Logs auf Fehler

### Problem: Docker-Container starten nicht

**LÃ¶sung:**
1. ÃœberprÃ¼fen Sie Docker-Logs:
   ```bash
   docker-compose logs
   ```
2. Stellen Sie sicher, dass alle Ports verfÃ¼gbar sind
3. ÃœberprÃ¼fen Sie die `.env`-Datei

### Problem: "Tabelle existiert nicht"

**LÃ¶sung:**
FÃ¼hren Sie einen Crawler aus, um die Tabelle zu erstellen:
```bash
cd heise
python3 main.py
```

---

## ğŸ—‚ï¸ Beispiele & Screenshots

(mit Tableau und DeepNote, Stand MÃ¤rz 2025)

![image](https://github.com/user-attachments/assets/ce6ceae0-bdf4-499c-9577-973017bb1eff)

![image](https://github.com/user-attachments/assets/3affd472-8475-4534-99e6-54500493418c)

![image](https://github.com/user-attachments/assets/984babc4-d264-44be-8534-17fdae1f8d5f)

![image](https://github.com/user-attachments/assets/0c1d7a13-0f28-497c-afb3-048ee0a309e7)

![image](https://github.com/user-attachments/assets/ba9a3180-4ae8-4ab3-b4ae-3e81f4621c23)

![image](https://github.com/user-attachments/assets/85ecd8a3-1f31-49d0-ae3a-efdfd98bef21)

![image](https://github.com/user-attachments/assets/1d5c57f7-72be-4aca-8f03-d4fba8bfba9d)

![image](https://github.com/user-attachments/assets/cde65d2c-2b22-481d-9ba4-1c4086eb3f23)

![image](https://github.com/user-attachments/assets/10c87c9c-d444-487c-992f-73d3d4b4a185)

### Deepnote:

Wir haben auch einige Graphen mit [Deepnote](https://deepnote.com/app/schachner/Web-Crawler-d5025a36-3829-4c12-ad2d-b81aa84bd217?utm_source=app-settings&utm_medium=product-embed&utm_campaign=data-app&utm_content=d5025a36-3829-4c12-ad2d-b81aa84bd217&__embedded=true) generiert (â— nur mit zufÃ¤lligen 10.000 Zeilen â—)

![image](https://github.com/user-attachments/assets/ea99ead8-0b48-47d0-8ddc-7c8ce3bd6b53)

Schauen Sie sich auch die [data/Datamining_Heise web crawler-3.twb](https://github.com/SchBenedikt/datamining/blob/3f3fe413aeff25a1ae024215745ed6fa82fc2add/data/Datamining_Heise%20web%20crawler-3.twb)-Datei mit einem Auszug von Analysen an.

---

## ğŸ“œ Lizenz

Dieses Programm ist lizenziert unter **GNU GENERAL PUBLIC LICENSE**

Siehe [LICENSE](LICENSE) fÃ¼r weitere Details.

---

## ğŸ™‹ Ãœber uns

Dieses Projekt wurde von uns beiden innerhalb weniger Tage programmiert und wird stÃ¤ndig weiterentwickelt:
- https://github.com/schBenedikt
- https://github.com/schVinzenz

### ğŸ“¬ Kontakt

ZÃ¶gern Sie nicht, uns zu kontaktieren, wenn Sie Fragen, Feedback haben oder einfach nur Hallo sagen mÃ¶chten!

ğŸ“§ E-Mail: [server@schÃ¤chner.de](mailto:server@schÃ¤chner.de)

ğŸŒ Website:
- https://technik.schÃ¤chner.de
- https://benedikt.schÃ¤chner.de
- https://vinzenz.schÃ¤chner.de

### ğŸ’– Besonderer Dank

Die Idee fÃ¼r unseren Heise News Crawler stammt von David Kriesel und seiner PrÃ¤sentation "Spiegel Mining" auf dem 33c3.

---

**Happy Crawling! ğŸ‰**

````
