````markdown
# üèóÔ∏è System-Architektur

## √úbersicht

Das Unified News Mining System ist ein vollst√§ndig integriertes Crawler-System mit separaten Datenbanktabellen, einem einheitlichen Dashboard und zentraler Verwaltung √ºber Docker.

---

## üìê Architektur-Diagramm

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  UNIFIED NEWS MINING SYSTEM                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Heise Crawlers     ‚îÇ              ‚îÇ   Chip Crawlers      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Archive Crawler      ‚îÇ              ‚îÇ Archive Crawler      ‚îÇ
‚îÇ (r√ºckw√§rts)          ‚îÇ              ‚îÇ (vorw√§rts)           ‚îÇ
‚îÇ - Start: 2025/10     ‚îÇ              ‚îÇ - Start: Seite 1     ‚îÇ
‚îÇ - Ziel: 2000/01      ‚îÇ              ‚îÇ - Ziel: Letzte Seite ‚îÇ
‚îÇ                      ‚îÇ              ‚îÇ                      ‚îÇ
‚îÇ Live Crawler         ‚îÇ              ‚îÇ Live Crawler         ‚îÇ
‚îÇ (alle 5 Minuten)     ‚îÇ              ‚îÇ (alle 10 Minuten)    ‚îÇ
‚îÇ - Pr√ºft: Aktuellen   ‚îÇ              ‚îÇ - Pr√ºft: Seite 1     ‚îÇ
‚îÇ   Monat              ‚îÇ              ‚îÇ   (neueste)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                                     ‚îÇ
           ‚îÇ INSERT INTO heise                   ‚îÇ INSERT INTO chip
           ‚îÇ (title, url, date,                  ‚îÇ (title, url, date,
           ‚îÇ  author, category,                  ‚îÇ  author, keywords,
           ‚îÇ  keywords, ...)                     ‚îÇ  description, ...)
           ‚îÇ                                     ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   PostgreSQL DB      ‚îÇ
              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
              ‚îÇ                      ‚îÇ
              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
              ‚îÇ  ‚îÇ  heise table   ‚îÇ  ‚îÇ
              ‚îÇ  ‚îÇ  (10 columns)  ‚îÇ  ‚îÇ
              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
              ‚îÇ                      ‚îÇ
              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
              ‚îÇ  ‚îÇ  chip table    ‚îÇ  ‚îÇ
              ‚îÇ  ‚îÇ  (12 columns)  ‚îÇ  ‚îÇ
              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
              ‚îÇ                      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ               ‚îÇ
         ‚ñº               ‚ñº               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Streamlit   ‚îÇ  ‚îÇ Discord Bot ‚îÇ  ‚îÇ Export      ‚îÇ
‚îÇ Dashboard   ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ Tools       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Merge     ‚îÇ  ‚îÇ ‚Ä¢ Heise     ‚îÇ  ‚îÇ heise/      ‚îÇ
‚îÇ   beide     ‚îÇ  ‚îÇ   stats     ‚îÇ  ‚îÇ export_     ‚îÇ
‚îÇ   Tabellen  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ articles.py ‚îÇ
‚îÇ             ‚îÇ  ‚îÇ ‚Ä¢ Chip      ‚îÇ  ‚îÇ             ‚îÇ
‚îÇ ‚Ä¢ Filter:   ‚îÇ  ‚îÇ   stats     ‚îÇ  ‚îÇ chip/       ‚îÇ
‚îÇ   - Quelle  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ export_     ‚îÇ
‚îÇ   - Datum   ‚îÇ  ‚îÇ ‚Ä¢ Heute     ‚îÇ  ‚îÇ articles.py ‚îÇ
‚îÇ   - Autor   ‚îÇ  ‚îÇ   & Total   ‚îÇ  ‚îÇ             ‚îÇ
‚îÇ   - Kat.    ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ Formate:    ‚îÇ
‚îÇ             ‚îÇ  ‚îÇ ‚Ä¢ Updates   ‚îÇ  ‚îÇ ‚Ä¢ CSV       ‚îÇ
‚îÇ ‚Ä¢ 20+       ‚îÇ  ‚îÇ   alle 10   ‚îÇ  ‚îÇ ‚Ä¢ XLSX      ‚îÇ
‚îÇ   Viz.      ‚îÇ  ‚îÇ   Minuten   ‚îÇ  ‚îÇ ‚Ä¢ JSON      ‚îÇ
‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ ‚Ä¢ SQL       ‚îÇ
‚îÇ ‚Ä¢ Export    ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ
‚îÇ ‚Ä¢ AI        ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ
‚îÇ   Analytics ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     DOCKER COMPOSE STACK                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  heise-archive-crawler   ‚îÇ  heise-live-crawler                  ‚îÇ
‚îÇ  chip-archive-crawler    ‚îÇ  chip-live-crawler                   ‚îÇ
‚îÇ  streamlit-dashboard     ‚îÇ  discord-bot                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  DOCKER MANAGEMENT TOOLS (Optional)              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Portainer (Port 9000)   ‚îÇ  Dockge (Port 5001)                  ‚îÇ
‚îÇ  - Container starten/stoppen/pausieren                          ‚îÇ
‚îÇ  - Logs in Echtzeit ansehen                                     ‚îÇ
‚îÇ  - Ressourcen-Monitoring                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Datenfluss

### 1. Crawling-Phase

```
Heise Archive Crawler:
  https://www.heise.de/newsticker/archiv/2025/10
  ‚îî‚îÄ> Extrahiert Artikel-Metadaten
      ‚îî‚îÄ> Speichert in heise-Tabelle
          ‚îî‚îÄ> Geht zu 2025/09, 2025/08, ...

Heise Live Crawler:
  Jede 5 Minuten:
  ‚îî‚îÄ> Pr√ºft aktuellen Monat
      ‚îî‚îÄ> Findet neue Artikel
          ‚îî‚îÄ> Speichert nur Neue (Duplikate-Check via URL)

Chip Archive Crawler:
  https://www.chip.de/news/?p=1
  ‚îî‚îÄ> Extrahiert Artikel-Metadaten
      ‚îî‚îÄ> Speichert in chip-Tabelle
          ‚îî‚îÄ> Geht zu Seite 2, 3, 4, ...

Chip Live Crawler:
  Jede 10 Minuten:
  ‚îî‚îÄ> Pr√ºft Seite 1 (neueste Artikel)
      ‚îî‚îÄ> Findet neue Artikel
          ‚îî‚îÄ> Speichert nur Neue (Duplikate-Check via URL)
```

### 2. Datenbank-Phase

```
PostgreSQL Datenbank:
  ‚îú‚îÄ> heise-Tabelle
  ‚îÇ   ‚îú‚îÄ> id, title, url, date, author
  ‚îÇ   ‚îú‚îÄ> category, keywords, word_count
  ‚îÇ   ‚îî‚îÄ> editor_abbr, site_name
  ‚îÇ
  ‚îî‚îÄ> chip-Tabelle
      ‚îú‚îÄ> id, url, title, author, date
      ‚îú‚îÄ> keywords, description, type
      ‚îî‚îÄ> page_level1, page_level2, page_level3, page_template
```

### 3. Visualisierungs-Phase

```
Streamlit Dashboard:
  ‚îî‚îÄ> SELECT * FROM heise
  ‚îî‚îÄ> SELECT * FROM chip
      ‚îî‚îÄ> pd.concat([df_heise, df_chip])
          ‚îî‚îÄ> Filter nach Quelle
              ‚îî‚îÄ> Visualisierungen:
                  ‚îú‚îÄ> Autoren-Netzwerke
                  ‚îú‚îÄ> Keyword-Analysen
                  ‚îú‚îÄ> Zeitanalysen
                  ‚îú‚îÄ> AI-Analysen
                  ‚îî‚îÄ> Export-Funktionen
```

### 4. Benachrichtigungs-Phase

```
Discord Bot:
  ‚îî‚îÄ> Jede 10 Minuten:
      ‚îú‚îÄ> SELECT COUNT(*) FROM heise
      ‚îú‚îÄ> SELECT COUNT(*) FROM chip
      ‚îî‚îÄ> Postet Statistiken im Discord-Channel

E-Mail-Benachrichtigungen:
  ‚îî‚îÄ> Bei Fehlern:
      ‚îî‚îÄ> Sendet Alert an ALERT_EMAIL
```

---

## üìä Datenbankschema

### Heise-Tabelle

```sql
CREATE TABLE IF NOT EXISTS heise (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    url TEXT UNIQUE NOT NULL,
    date TEXT,
    author TEXT,
    category TEXT,
    keywords TEXT,
    word_count INTEGER,
    editor_abbr TEXT,
    site_name TEXT
);

CREATE INDEX idx_heise_date ON heise(date);
CREATE INDEX idx_heise_author ON heise(author);
CREATE INDEX idx_heise_category ON heise(category);
```

**Beispieldaten:**
```json
{
  "id": 1,
  "title": "Neue KI-Technologie revolutioniert...",
  "url": "https://www.heise.de/news/...",
  "date": "2025-10-02T10:30:00",
  "author": "Max Mustermann",
  "category": "K√ºnstliche Intelligenz",
  "keywords": "KI, Machine Learning, Innovation",
  "word_count": 450,
  "editor_abbr": "mm",
  "site_name": "heise online"
}
```

### Chip-Tabelle

```sql
CREATE TABLE IF NOT EXISTS chip (
    id SERIAL PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    title TEXT NOT NULL,
    author TEXT,
    date TEXT,
    keywords TEXT,
    description TEXT,
    type TEXT,
    page_level1 TEXT,
    page_level2 TEXT,
    page_level3 TEXT,
    page_template TEXT
);

CREATE INDEX idx_chip_date ON chip(date);
CREATE INDEX idx_chip_author ON chip(author);
CREATE INDEX idx_chip_type ON chip(type);
```

**Beispieldaten:**
```json
{
  "id": 1,
  "url": "https://www.chip.de/news/...",
  "title": "Smartphone-Test 2025: Die besten...",
  "author": "CHIP Redaktion",
  "date": "2025-10-02",
  "keywords": "Smartphone, Test, Vergleich",
  "description": "Im gro√üen Vergleichstest...",
  "type": "Test",
  "page_level1": "News",
  "page_level2": "Mobilfunk",
  "page_level3": "Smartphones",
  "page_template": "article"
}
```

---

## üîß Komponenten-Details

### Heise Crawler

**Datei:** `heise/main.py` (Archive), `heise/current_crawler.py` (Live)

**Funktionsweise:**
1. L√§dt Archive-Seite: `https://www.heise.de/newsticker/archiv/YYYY/MM`
2. Parst HTML mit BeautifulSoup
3. Extrahiert Artikel-Links und Metadaten
4. Pr√ºft Duplikate via URL
5. Speichert neue Artikel in `heise`-Tabelle
6. Bei < 10 Artikel/Tag: E-Mail-Alert

**Besonderheiten:**
- R√ºckw√§rts-Crawling (neueste zu √§lteste)
- Live-Crawler pr√ºft nur aktuellen Monat
- Erkennt Editor-K√ºrzel (z.B. "mm", "js")
- Erfasst Wortanzahl

### Chip Crawler

**Datei:** `chip/main.py` (Archive), `chip/current_crawler.py` (Live)

**Funktionsweise:**
1. L√§dt News-Seite: `https://www.chip.de/news/?p=PAGE`
2. Parst HTML mit BeautifulSoup
3. Extrahiert Artikel-Links und Metadaten aus `<script type="application/ld+json">`
4. Pr√ºft Duplikate via URL
5. Speichert neue Artikel in `chip`-Tabelle

**Besonderheiten:**
- Vorw√§rts-Crawling (Seite 1 zu Seite N)
- Live-Crawler pr√ºft nur Seite 1
- Extrahiert strukturierte Daten (JSON-LD)
- Erfasst Page-Hierarchie (Level 1-3)

### Streamlit Dashboard

**Datei:** `visualization/streamlit_app.py`

**Funktionsweise:**
1. L√§dt Daten aus beiden Tabellen
2. F√ºgt `source`-Spalte hinzu ('heise' oder 'chip')
3. Merged DataFrames: `pd.concat([df_heise, df_chip])`
4. Bietet Filter-Optionen in Sidebar
5. Generiert Visualisierungen on-the-fly
6. Cached Daten f√ºr Performance

**Features:**
- **√úbersicht:** KPIs, Statistiken, Trends
- **Zeitanalysen:** Artikel pro Tag/Woche/Monat
- **Autoren-Netzwerke:** NetworkX + Plotly
- **Keyword-Analysen:** Top Keywords, Trends
- **Word Clouds:** H√§ufigste Begriffe
- **AI-Analysen:** Topic Modeling, Sentiment
- **Suchfunktion:** Volltext-Suche
- **Export:** CSV, Excel, JSON, SQL

### Discord Bot

**Datei:** `heise/bot.py`

**Funktionsweise:**
1. Verbindet zu Discord
2. Jede 10 Minuten:
   - Z√§hlt Artikel in beiden Tabellen
   - Z√§hlt heutige Artikel
   - Z√§hlt Autoren
3. Postet Embed-Message mit Statistiken

**Ausgabe:**
```
üìä News Mining Statistik

üì∞ Artikel heute: 45 (Heise: 25, Chip: 20)
üìö Artikel gesamt: 12.345 (Heise: 8.000, Chip: 4.345)
‚úçÔ∏è Autoren gesamt: 234

Stand: 02.10.2025 10:30
```

### Export-Tools

**Dateien:** `heise/export_articles.py`, `chip/export_articles.py`

**Funktionsweise:**
1. Verbindet zur Datenbank
2. Liest alle Artikel der jeweiligen Tabelle
3. Konvertiert zu gew√ºnschtem Format
4. Speichert in `data/`-Verzeichnis

**Formate:**
- **CSV:** `data/heise_articles_YYYYMMDD.csv`
- **Excel:** `data/heise_articles_YYYYMMDD.xlsx`
- **JSON:** `data/heise_articles_YYYYMMDD.json`
- **SQL:** `data/heise_articles_YYYYMMDD.sql`

---

## üê≥ Docker-Architektur

### Docker Compose Services

```yaml
services:
  heise-archive-crawler:
    - F√ºhrt heise/main.py aus
    - R√ºckw√§rts-Crawling
    - Restart: unless-stopped
    
  heise-live-crawler:
    - F√ºhrt heise/current_crawler.py aus
    - Pr√ºft alle 5 Minuten
    - Restart: unless-stopped
    
  chip-archive-crawler:
    - F√ºhrt chip/main.py aus
    - Vorw√§rts-Crawling
    - Restart: unless-stopped
    
  chip-live-crawler:
    - F√ºhrt chip/current_crawler.py aus
    - Pr√ºft alle 10 Minuten
    - Restart: unless-stopped
    
  streamlit-dashboard:
    - F√ºhrt streamlit run aus
    - Port 8501 exposed
    - Volumes f√ºr Code-Updates
    
  discord-bot:
    - F√ºhrt heise/bot.py aus
    - Postet alle 10 Minuten
    - Restart: unless-stopped
```

### Docker-Netzwerk

```
crawler-network (bridge):
  ‚îú‚îÄ> heise-archive-crawler
  ‚îú‚îÄ> heise-live-crawler
  ‚îú‚îÄ> chip-archive-crawler
  ‚îú‚îÄ> chip-live-crawler
  ‚îú‚îÄ> streamlit-dashboard
  ‚îî‚îÄ> discord-bot
```

Alle Container k√∂nnen sich √ºber dieses Netzwerk erreichen und teilen die gleiche `.env`-Datei.

---

## üîê Sicherheit

### Umgebungsvariablen

Sensible Daten werden √ºber `.env`-Datei verwaltet:
- Niemals in Git committen (`.gitignore`)
- Nur lesbar f√ºr Container
- Verschl√ºsselte √úbertragung (SMTP SSL/TLS)

### Datenbank-Sicherheit

- PostgreSQL-Zugriff nur √ºber Credentials
- Unique Constraints verhindern Duplikate
- Prepared Statements gegen SQL-Injection
- Index auf h√§ufig abgefragte Spalten

### API-Sicherheit

- Keine Authentifizierung (lokaler Zugriff)
- Bei √∂ffentlichem Deployment: OAuth/JWT empfohlen
- Rate-Limiting f√ºr API-Endpoints

---

## üìà Skalierbarkeit

### Horizontale Skalierung

**Weitere Quellen hinzuf√ºgen:**
1. Neuen Ordner erstellen (z.B. `golem/`)
2. Crawler-Skripte kopieren und anpassen
3. Neue Tabelle in DB erstellen
4. Service zu `docker-compose.yml` hinzuf√ºgen
5. Streamlit l√§dt automatisch neue Tabelle

**Beispiel:**
```yaml
golem-live-crawler:
  build: .
  container_name: golem-live-crawler
  command: python3 golem/current_crawler.py
  ...
```

### Vertikale Skalierung

**Performance-Optimierungen:**
- Datenbank-Indizes auf h√§ufig abgefragte Spalten
- Streamlit-Caching f√ºr gro√üe Datasets
- Batch-Inserts statt einzelner INSERTs
- Connection Pooling f√ºr Datenbank

### Load Balancing

**Bei hoher Last:**
- Mehrere Streamlit-Instanzen hinter Nginx
- PostgreSQL Read Replicas
- Redis f√ºr Session-Management
- CDN f√ºr statische Assets

---

## üîÑ Erweiterbarkeit

### Plugin-Architektur

Das System ist modular aufgebaut:

```
plugins/
‚îú‚îÄ‚îÄ crawlers/
‚îÇ   ‚îú‚îÄ‚îÄ heise_crawler.py
‚îÇ   ‚îú‚îÄ‚îÄ chip_crawler.py
‚îÇ   ‚îî‚îÄ‚îÄ custom_crawler.py  <- Neuer Crawler
‚îÇ
‚îú‚îÄ‚îÄ exporters/
‚îÇ   ‚îú‚îÄ‚îÄ csv_exporter.py
‚îÇ   ‚îú‚îÄ‚îÄ json_exporter.py
‚îÇ   ‚îî‚îÄ‚îÄ pdf_exporter.py    <- Neuer Exporter
‚îÇ
‚îî‚îÄ‚îÄ visualizations/
    ‚îú‚îÄ‚îÄ network_graph.py
    ‚îú‚îÄ‚îÄ time_series.py
    ‚îî‚îÄ‚îÄ custom_viz.py      <- Neue Visualisierung
```

### API-Endpunkte

**Bestehende:**
- `/stats` - Gesamtstatistiken
- `/articles` - Alle Artikel

**Erweiterbar:**
- `/api/v1/heise/articles` - Nur Heise
- `/api/v1/chip/articles` - Nur Chip
- `/api/v1/search?q=keyword` - Suche
- `/api/v1/authors` - Autoren-Liste
- `/api/v1/keywords` - Keyword-Trends

---

## üéØ Best Practices

### Crawler

1. **Rate Limiting:** Pause zwischen Requests (1-2 Sekunden)
2. **User-Agent:** Identifizierbar als Bot
3. **Robots.txt:** Respektieren der Crawling-Regeln
4. **Error Handling:** Graceful Degradation bei Fehlern
5. **Logging:** Ausf√ºhrliche Logs f√ºr Debugging

### Datenbank

1. **Normalisierung:** Separate Tabellen f√ºr bessere Performance
2. **Indizes:** Auf h√§ufig abgefragte Spalten
3. **Backups:** Regelm√§√üige Datenbank-Backups
4. **Constraints:** UNIQUE auf URL verhindert Duplikate
5. **Transactions:** ACID-Eigenschaften nutzen

### Streamlit

1. **Caching:** `@st.cache_data` f√ºr teure Operationen
2. **Lazy Loading:** Gro√üe Datasets erst bei Bedarf laden
3. **Pagination:** Bei sehr vielen Artikeln
4. **Responsive:** Mobile-freundliches Layout
5. **Error Handling:** Try-Except f√ºr alle DB-Queries

---

## üõ†Ô∏è Monitoring & Debugging

### Logs

```bash
# Docker Logs
docker-compose logs -f [service-name]

# Spezifischer Crawler
docker-compose logs -f heise-live-crawler

# Alle Services
docker-compose logs -f
```

### Metriken

**Wichtige KPIs:**
- Artikel pro Tag
- Crawler-Erfolgsrate
- Duplikate-Erkennungsrate
- API-Response-Zeit
- Streamlit-Load-Zeit

### Alerts

**E-Mail-Benachrichtigungen bei:**
- Weniger als 10 Artikel/Tag
- Datenbank-Verbindungsfehler
- Crawler-Crashes
- Disk Space < 10%

---

## üöÄ Deployment-Optionen

### Option 1: Lokales Deployment

```bash
# Crawlers manuell starten
python3 heise/main.py
python3 chip/main.py

# Streamlit starten
streamlit run visualization/streamlit_app.py
```

### Option 2: Docker Deployment

```bash
# Alle Services starten
docker-compose up -d

# Logs √ºberwachen
docker-compose logs -f
```

### Option 3: Cloud Deployment (AWS/GCP/Azure)

**Empfohlene Architektur:**
- EC2/Compute Engine/VM f√ºr Container
- RDS/Cloud SQL/Azure DB f√ºr PostgreSQL
- CloudWatch/Logging f√ºr Monitoring
- S3/Cloud Storage f√ºr Exports
- Load Balancer f√ºr Streamlit

---

## üìö Weiterf√ºhrende Dokumentation

- **[README.md](README.md)** - Hauptdokumentation
- **[QUICKSTART.md](QUICKSTART.md)** - Schnellstart-Anleitung
- **[DOCKER_SETUP.md](DOCKER_SETUP.md)** - Docker-Setup-Details
- **[SECURITY.md](SECURITY.md)** - Sicherheitsrichtlinien

---

## ü§ù Beitr√§ge

Beitr√§ge sind willkommen! Bitte √∂ffnen Sie ein Issue oder Pull Request auf GitHub.

**Contribution Guidelines:**
1. Fork das Repository
2. Erstellen Sie einen Feature-Branch
3. Committen Sie Ihre √Ñnderungen
4. Pushen Sie zum Branch
5. √ñffnen Sie einen Pull Request

---

**Stand:** Oktober 2025  
**Version:** 2.0 (Separate Tables Architecture)  
**Status:** ‚úÖ Production Ready

````